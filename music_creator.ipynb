{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4406340",
   "metadata": {},
   "source": [
    "# Music Generation With Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0d7e6",
   "metadata": {},
   "source": [
    "## All nescessary imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "45dce235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21 as m21\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import json\n",
    "m21.environment.set('musescoreDirectPNGPath', '/Applications/MuseScore 4.app/Contents/MacOS/mscore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3aef42",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "1e2ea141",
   "metadata": {},
   "outputs": [],
   "source": [
    "SONG_DIR = 'erk'\n",
    "FILE_EXSTENSION = 'krn'\n",
    "NOTE_LENGTHS = [i*0.25 for i in range(1, 17)]\n",
    "SEQUENCE_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16fac7",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### Read songs from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "b9756a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_songs_from_dir(song_dir, file_exstension):\n",
    "    \"\"\"\n",
    "    Returns a list of all songs with correct file exstension from a directory\n",
    "    \n",
    "    Input params\n",
    "    song_dir: Directory for the songs\n",
    "    file_exstension: The fileformat for the songs (in case of more files in directory)\n",
    "    \n",
    "    Returns\n",
    "    songs: List of all songs as music21 score objects\n",
    "    \"\"\"\n",
    "    songs = []\n",
    "    for path, subdirs, files in os.walk(song_dir):\n",
    "        for file in [i for i in files if i[-3:] == file_exstension]:\n",
    "            song = m21.converter.parse(path+'/'+ file)\n",
    "            songs.append(song)\n",
    "    return songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f085c99",
   "metadata": {},
   "source": [
    "### Check if all notes in a song are of the acceptable length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "d6dfbae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_length(song, note_length):\n",
    "    '''\n",
    "    Check if all notes in a song are of the acceptable length.\n",
    "    \n",
    "    Input params\n",
    "    song: Song as music 21 object\n",
    "    note_length: List of acceptable note length in quarternotes\n",
    "    \n",
    "    Returns:\n",
    "    True if all notes are correct length\n",
    "    False if any note is not correct length\n",
    "    '''\n",
    "    for note in song.flat.notesAndRests:\n",
    "        if note.duration.quarterLength not in (note_length):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d06ae",
   "metadata": {},
   "source": [
    "### Get key of song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e7ba02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(song):\n",
    "    '''\n",
    "    Function that returns the key of a song, if not found, then calculated instead.\n",
    "    \n",
    "    Input params\n",
    "    song: Song as music 21object\n",
    "    \n",
    "    Returns\n",
    "    key: Key as music21 object\n",
    "    '''\n",
    "    parts = song.getElementsByClass(m21.stream.Part)\n",
    "    measures_part0 = parts[0].getElementsByClass(m21.stream.Measure)\n",
    "    key = measures_part0[0][4]\n",
    "    if not isinstance(key, m21.key.Key):\n",
    "        key = song.analyze(\"key\")\n",
    "    return key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a1299",
   "metadata": {},
   "source": [
    "### Transpose song to C/Am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "6adaf7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_song(song, key):\n",
    "    '''\n",
    "    Transposes song to C if major, Am if minor\n",
    "    \n",
    "    Input params:\n",
    "    song: Song as music21 object\n",
    "    key: Key as music21 object\n",
    "    \n",
    "    Returns:\n",
    "    song_transposed: Song in C major or A minor as music21 object\n",
    "    '''\n",
    "    if key.mode == 'minor':\n",
    "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch('A'))\n",
    "    else:\n",
    "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch('C'))\n",
    "    song_transposed = song.transpose(interval)\n",
    "    return song_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3318b1f",
   "metadata": {},
   "source": [
    "### Converts song into a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "497f8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_to_time_series(song):\n",
    "    '''\n",
    "    Adds a symbol for every pitch or rest. Depending of the duration of the note/rest, '-' are added \n",
    "    for every 16th note it's ringing.\n",
    "    \n",
    "    Input params\n",
    "    song: Song as music21 object\n",
    "    \n",
    "    Returns\n",
    "    song_time_series: List with all songs as time series\n",
    "    '''\n",
    "    song_time_series = []\n",
    "    for note in song.flat.notesAndRests:\n",
    "        if isinstance(note, m21.note.Note):\n",
    "            symbol = note.pitch.midi\n",
    "        elif isinstance(note, m21.note.Rest):\n",
    "            symbol = 'R'\n",
    "        length = int(note.duration.quarterLength/0.25)\n",
    "        song_time_series.append(symbol)\n",
    "        if length > 1:\n",
    "            for i in range(length-1):\n",
    "                song_time_series.append('-')\n",
    "    return song_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581c074",
   "metadata": {},
   "source": [
    "### Map the notes to integers and save that dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "abc94373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_notes(songs_time_series):\n",
    "    '''\n",
    "    Maps all the elements in the time series list to integers, and saves that dict\n",
    "    \n",
    "    Input params\n",
    "    songs_time_series: List of songs in time series format\n",
    "    \n",
    "    Returns\n",
    "    mapping: Mapping as dictionary\n",
    "    '''\n",
    "    songs = [note for song in songs_time_series for note in song]\n",
    "    notes = list(set(songs))\n",
    "    mapping = {}\n",
    "    for i, note in enumerate(notes):\n",
    "        mapping[note] = i\n",
    "    with open('mapping_file', \"w\") as fp:\n",
    "        json.dump(mapping, fp, indent=4)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63889378",
   "metadata": {},
   "source": [
    "### Creates a new list of mapped songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "38461204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_songs(songs_time_series, mapping):\n",
    "    '''\n",
    "    Converts the time series list of songs into a list of mapped songs\n",
    "    \n",
    "    Input params\n",
    "    songs_time_series: list of songs in time series format\n",
    "    mapping: dictionary with every note in songs_time_series mapped to an int\n",
    "    \n",
    "    Returns\n",
    "    mapped_songs: list of songs mapped\n",
    "    '''\n",
    "    mapped_songs = []\n",
    "    for song in songs_time_series:\n",
    "        mapped_song = []\n",
    "        for note in song:\n",
    "            mapped_note = mapping[note]\n",
    "            mapped_song.append(mapped_note)\n",
    "        mapped_songs.append(mapped_song)\n",
    "    return mapped_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5641975",
   "metadata": {},
   "source": [
    "### Creates training data as inputs and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a9f8bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_target_data(mapped_songs, sequence_size):\n",
    "    '''\n",
    "    Takes the mapped songs and a sequence size to create inputs sequences and target values. Target value would\n",
    "    be the note after the sequence.\n",
    "    \n",
    "    Input params\n",
    "    mapped_songs: list of mapped songs\n",
    "    sequence_size: int with length of training sequences\n",
    "    \n",
    "    Returns\n",
    "    X: Sequence of notes with sequence_size length\n",
    "    y: Target for sequence\n",
    "    '''\n",
    "    inputs = []\n",
    "    target = []\n",
    "    for song in mapped_songs:\n",
    "        for i in range(len(song) - sequence_size):\n",
    "            inputs.append(song[i:i + sequence_size])\n",
    "            target.append(song[i + sequence_size])\n",
    "    X = keras.utils.to_categorical(inputs, num_classes=len(mapping))\n",
    "    y = np.array(target)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bfea8a",
   "metadata": {},
   "source": [
    "### Creates lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "b22c6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nbr_outputs,nbr_units):\n",
    "    '''\n",
    "    Creates a three hidden layer model with 2 lstm and one dropout layer. Uses accuracy for metrics, \n",
    "    sparse_categorical_crossentropy for loss, Adam optimizer, and activation function softmax.\n",
    "    \n",
    "    Input params\n",
    "    nbr_outputs: The number of outputs the model should have\n",
    "    nbr_units: The number of units for the lstm layers\n",
    "    \n",
    "    Returns\n",
    "    model: Keras Tensorflow neural network model\n",
    "    '''\n",
    "    inputs = keras.layers.Input(shape=(None, nbr_outputs))\n",
    "    lstm1 = keras.layers.LSTM(nbr_units, return_sequences=True)(inputs)\n",
    "    lstm2 = keras.layers.LSTM(nbr_units)(lstm1)\n",
    "    dropout = keras.layers.Dropout(0.2)(lstm2) \n",
    "    output = keras.layers.Dense(nbr_outputs, activation=\"softmax\")(dropout)\n",
    "\n",
    "    model = keras.Model(inputs, output)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f6d58",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "8e542da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_time_series = []\n",
    "for song in read_songs_from_dir(SONG_DIR, FILE_EXSTENSION):\n",
    "    if valid_length(song, NOTE_LENGTHS):\n",
    "        key = get_key(song)\n",
    "        transposed_song = transpose_song(song, key)\n",
    "        song_time_series = song_to_time_series(transposed_song)\n",
    "        songs_time_series.append(song_time_series)\n",
    "mapping = mapping_notes(songs_time_series)\n",
    "mapped_songs = map_songs(songs_time_series, mapping)\n",
    "X_train, y_train = create_input_target_data(mapped_songs, SEQUENCE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb57b13",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e383f2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_60 (InputLayer)       [(None, None, 21)]        0         \n",
      "                                                                 \n",
      " lstm_79 (LSTM)              (None, None, 512)         1093632   \n",
      "                                                                 \n",
      " lstm_80 (LSTM)              (None, 512)               2099200   \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 21)                10773     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3203605 (12.22 MB)\n",
      "Trainable params: 3203605 (12.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(len(mapping), 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32253a8",
   "metadata": {},
   "source": [
    "## Training the model and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "18fed0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 49s 665ms/step - loss: 1.4592 - accuracy: 0.6651\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 47s 646ms/step - loss: 1.3139 - accuracy: 0.6773\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 47s 654ms/step - loss: 1.2247 - accuracy: 0.6747\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 47s 651ms/step - loss: 1.0217 - accuracy: 0.6729\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 47s 657ms/step - loss: 0.9823 - accuracy: 0.6812\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 46s 644ms/step - loss: 0.9572 - accuracy: 0.6840\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 46s 640ms/step - loss: 0.9313 - accuracy: 0.6941\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 46s 641ms/step - loss: 0.9046 - accuracy: 0.6989\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 47s 652ms/step - loss: 0.8770 - accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 47s 651ms/step - loss: 0.8309 - accuracy: 0.7215\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 47s 647ms/step - loss: 0.8009 - accuracy: 0.7298\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 47s 653ms/step - loss: 0.7642 - accuracy: 0.7383\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 48s 665ms/step - loss: 0.7563 - accuracy: 0.7464\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 47s 656ms/step - loss: 0.7316 - accuracy: 0.7571\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 48s 662ms/step - loss: 0.7310 - accuracy: 0.7547\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 47s 660ms/step - loss: 0.6967 - accuracy: 0.7601\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 48s 671ms/step - loss: 0.6823 - accuracy: 0.7730\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 49s 681ms/step - loss: 0.7791 - accuracy: 0.7582\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 51s 712ms/step - loss: 1.1160 - accuracy: 0.6642\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 51s 715ms/step - loss: 1.0777 - accuracy: 0.6723\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 52s 720ms/step - loss: 1.0423 - accuracy: 0.6762\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 51s 714ms/step - loss: 1.0304 - accuracy: 0.6779\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 52s 721ms/step - loss: 1.0176 - accuracy: 0.6823\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 51s 716ms/step - loss: 0.9981 - accuracy: 0.6825\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 52s 717ms/step - loss: 0.9831 - accuracy: 0.6825\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 51s 715ms/step - loss: 0.9647 - accuracy: 0.6825\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 52s 729ms/step - loss: 0.9572 - accuracy: 0.6930\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 52s 728ms/step - loss: 0.9937 - accuracy: 0.6834\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 52s 727ms/step - loss: 0.9719 - accuracy: 0.6825\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 52s 724ms/step - loss: 0.9277 - accuracy: 0.6875\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 53s 730ms/step - loss: 0.9186 - accuracy: 0.6884\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 52s 724ms/step - loss: 0.9013 - accuracy: 0.6949\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 53s 736ms/step - loss: 0.9069 - accuracy: 0.6917\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 53s 741ms/step - loss: 0.8961 - accuracy: 0.6947\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 53s 733ms/step - loss: 0.8967 - accuracy: 0.6895\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 53s 734ms/step - loss: 0.8914 - accuracy: 0.6888\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 53s 735ms/step - loss: 0.8801 - accuracy: 0.6952\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 53s 732ms/step - loss: 0.8718 - accuracy: 0.6991\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 51s 710ms/step - loss: 0.8777 - accuracy: 0.7019\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 52s 716ms/step - loss: 0.8619 - accuracy: 0.7026\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 52s 723ms/step - loss: 0.8747 - accuracy: 0.7058\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 52s 719ms/step - loss: 0.8662 - accuracy: 0.7006\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 52s 728ms/step - loss: 0.8649 - accuracy: 0.7002\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 52s 715ms/step - loss: 0.8740 - accuracy: 0.7006\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 52s 722ms/step - loss: 0.8604 - accuracy: 0.7058\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 52s 722ms/step - loss: 0.8510 - accuracy: 0.7115\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 52s 725ms/step - loss: 0.8548 - accuracy: 0.7058\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 52s 723ms/step - loss: 0.8444 - accuracy: 0.7113\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 52s 726ms/step - loss: 0.8548 - accuracy: 0.7109\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 52s 720ms/step - loss: 0.8411 - accuracy: 0.7104\n",
      "INFO:tensorflow:Assets written to: model_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_4/assets\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size = 64)\n",
    "model.save('model_lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf46e8",
   "metadata": {},
   "source": [
    "## Functions for complex music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58f322",
   "metadata": {},
   "source": [
    "### Read songs from dir where song has many instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "29598362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_songs_from_dir_parts(song_dir, file_exstension):\n",
    "    \"\"\"\n",
    "    Returns a list of all songs as parts with correct file exstension from a directory\n",
    "    \n",
    "    Input params\n",
    "    song_dir: Directory for the songs\n",
    "    file_exstension: The fileformat for the songs (in case of more files in directory)\n",
    "    \n",
    "    Returns\n",
    "    songs: List of all songs as music21 parts objects\n",
    "    \"\"\"\n",
    "    songs = []\n",
    "    for path, subdirs, files in os.walk(song_dir):\n",
    "        for file in [i for i in files if i[-3:] == file_exstension]:\n",
    "            song = m21.converter.parse(path+'/'+ file)\n",
    "            parts = m21.instrument.partitionByInstrument(song)\n",
    "            songs.append(parts[0].recurse())         \n",
    "    return songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dccca8",
   "metadata": {},
   "source": [
    "### Get key for complex music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "447fafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_complex(song):\n",
    "    '''\n",
    "    Function that returns the key of a song, if not found, then calculated instead.\n",
    "    \n",
    "    Input params\n",
    "    song: Song as music21 part object\n",
    "    \n",
    "    Returns\n",
    "    key: Key as music21 object\n",
    "    '''\n",
    "    for part in song:\n",
    "        if isinstance(part, m21.key.Key):\n",
    "            return part\n",
    "        elif isinstance(part, m21.note.Note):\n",
    "            return song.analyze(\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812746f",
   "metadata": {},
   "source": [
    "### Two functions that converts the complex durations to 16th note dividable notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "1b45d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_value(target, values):\n",
    "    '''\n",
    "    Finds the closest value in a list to the target\n",
    "    \n",
    "    Input params\n",
    "    target: number to find the closest to in list\n",
    "    values: list of numbers\n",
    "    \n",
    "    Returns\n",
    "    closest_value: Closest value from the target in the list values\n",
    "    '''\n",
    "    closest_value = min(values, key=lambda x: abs(x - target))\n",
    "    return closest_value\n",
    "\n",
    "def convert_duration(song, note_length):\n",
    "    '''\n",
    "    Converts the duration of a note in a song, to the closest one in note_length list.\n",
    "    \n",
    "    Input params\n",
    "    song: song as music21 object\n",
    "    note_length: list of acceptable note lenghts\n",
    "    \n",
    "    Returns\n",
    "    song: song as music21 object with updated durations\n",
    "    '''\n",
    "    for note in song.flatten().notesAndRests:\n",
    "        if note.duration.quarterLength not in (note_length):\n",
    "            note.duration.quarterLength = find_closest_value(note.duration.quarterLength, note_length)\n",
    "    return song"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462a0ce",
   "metadata": {},
   "source": [
    "### Two functions to convert song to time series data, can handle chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "157eecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_chord(chord):\n",
    "    '''\n",
    "    Returns the highest note in the chord\n",
    "    \n",
    "    Input params\n",
    "    chord: chord as music21 object Chord\n",
    "    \n",
    "    Returns: highest pitch in chord in midi format\n",
    "    '''\n",
    "    return [note.pitch.midi for note in chord.notes][-1]\n",
    "\n",
    "def song_to_time_series_complex(song):\n",
    "    '''\n",
    "    Adds a symbol for every pitch or rest. Depending of the duration of the note/rest, '-' are added \n",
    "    for every 16th note it's ringing. Also handles chords\n",
    "    \n",
    "    Input params\n",
    "    song: Song as music21 object\n",
    "    \n",
    "    Returns\n",
    "    song_time_series: List with all songs as time series\n",
    "    '''\n",
    "    song_time_series = []\n",
    "    for note in song.flatten().notesAndRests:\n",
    "        if isinstance(note, m21.note.Note):\n",
    "            symbol = note.pitch.midi\n",
    "        elif isinstance(note, m21.note.Rest):\n",
    "            symbol = 'R'\n",
    "        elif note.isChord:\n",
    "            symbol = simplify_chord(note)\n",
    "        length = int(note.duration.quarterLength/0.25)\n",
    "        song_time_series.append(symbol)\n",
    "        if length > 1:\n",
    "            for i in range(length-1):\n",
    "                song_time_series.append('-')\n",
    "    return song_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e51247",
   "metadata": {},
   "source": [
    "## Global parameters for complex songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ff0fd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SONG_DIR = 'midi_songs'\n",
    "FILE_EXSTENSION = 'mid'\n",
    "NOTE_LENGTHS = [i*0.25 for i in range(1, 17)]#['1/3','2/3','1/12','1/6','5/12' ,'4/3'\n",
    "                #, '0.25', '0.5', '0.75', '1.0', '1.25', '1.5', '2.0', '3.0', '4.0']\n",
    "SEQUENCE_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce04ba5",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "4d66d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g8/7_44w75n3s312twwsd2rdyyh0000gn/T/ipykernel_85199/3702378589.py:26: StreamIteratorInefficientWarning: flatten is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  for note in song.flatten().notesAndRests:\n",
      "/var/folders/g8/7_44w75n3s312twwsd2rdyyh0000gn/T/ipykernel_85199/219652541.py:6: StreamIteratorInefficientWarning: transpose is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  song_transposed = song.transpose(interval)\n",
      "/var/folders/g8/7_44w75n3s312twwsd2rdyyh0000gn/T/ipykernel_85199/288021183.py:6: StreamIteratorInefficientWarning: analyze is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  return song.analyze(\"key\")\n"
     ]
    }
   ],
   "source": [
    "songs_time_series = []\n",
    "for song in read_songs_from_dir_parts(SONG_DIR, FILE_EXSTENSION):\n",
    "    key = get_key_complex(song)\n",
    "    if key == None:\n",
    "        continue\n",
    "    transposed_song = transpose_song(convert_duration(song,NOTE_LENGTHS), key)\n",
    "    song_time_series = song_to_time_series_complex(transposed_song)\n",
    "    songs_time_series.append(song_time_series)\n",
    "mapping = mapping_notes(songs_time_series)\n",
    "mapped_songs = map_songs(songs_time_series, mapping)\n",
    "X_train, y_train = create_input_target_data(mapped_songs, SEQUENCE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31772d6d",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "c3bb6f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_61 (InputLayer)       [(None, None, 86)]        0         \n",
      "                                                                 \n",
      " lstm_81 (LSTM)              (None, None, 400)         779200    \n",
      "                                                                 \n",
      " lstm_82 (LSTM)              (None, 400)               1281600   \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 400)               0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 86)                34486     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2095286 (7.99 MB)\n",
      "Trainable params: 2095286 (7.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(len(mapping), 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45d966",
   "metadata": {},
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size = 64)\n",
    "model.save('model_complex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
